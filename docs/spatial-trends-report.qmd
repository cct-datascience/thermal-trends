---
title: "Spatial Trends Through Time"
author: "Eric R. Scott"
format: html
editor: visual
execute: 
  echo: false
bibliography: references.bib
---

```{r}
#| label: setup
#| include: false
library(targets)
library(knitr)
library(here)
library(car)
library(terra)
library(tidyverse)
library(ggpattern)
library(tidyterra)
library(mgcv)
library(gratia)
library(sf)
library(colorspace)
library(patchwork)
library(flextable)
library(marginaleffects)
theme_set(theme_minimal())
tar_load(gdd_doy_stack_50, store = here("_targets/"))
```

The goal is to understand spatial and temporal trends in phenology using data on the DOY that various threshold GDD are reached in the Northeastern US.

::: callout-caution
All results presented here are extremely preliminary and subject to change!
:::

## Data

For this example, I'll use the 50 GDD threshold data.
For the sake of computational efficiency for this example, I'm going to downscale the data quite a bit, but the general evaluation should apply to the full resolution data.

```{r}
#downscale and extract as data frame
gdd_df <- 
  gdd_doy_stack_50 |>
  aggregate(fact = 12) |> 
  as_tibble(xy = TRUE, na.rm = TRUE) |> 
  pivot_longer(c(-x, -y), names_to = "year", values_to = "DOY", names_transform = list(year = as.integer)) |>
  mutate(year_scaled = year - min(year))
```

## Pixel-wise regression

A simple option is to run a regression for every pixel in the map, with one data point per year per GDD threshold.
We can then map the slopes easily (@fig-trend-map).

```{r}
#| label: fig-trend-map
#| fig-cap: "Map of slopes from pixel-wise linear regressions of DOY to reach 50 GDD over time from 1981 to 2023"
lm_df <- 
  gdd_df |> 
  nest(.by = c(x, y)) |> 
  mutate(mod = purrr::map(data, \(.x) lm(DOY ~ year_scaled, data = .x))) |> 
  mutate(resid = purrr::map(mod, \(.x) resid(.x))) |> 
  mutate(slope = purrr::map_dbl(mod, \(.x) coef(.x)[2]),
         p_val = purrr::map_dbl(mod, \(.x) broom::glance(.x)$p.value)) |> 
  mutate(p_val_adj = p.adjust(p_val, method = "BY"))

ggplot(lm_df, aes(x = x, y = y, fill = slope)) +
  geom_raster() +
  scale_fill_continuous_diverging(rev = TRUE) +
  coord_sf(crs = crs(gdd_doy_stack_50)) +
  labs(fill = "∆DOY/yr", x = "", y = "", title = "Trend in DOY to reach 50 GDD")
```

If we wish to make inferences based on these slopes, we may wish to know which are significantly different from zero.
Because we are dealing with thousands of p-values (`{r} nrow(as_tibble(gdd_doy_stack_50, na.rm = TRUE))` to be specific) and thus the probability of false-positives is quite high.
Whenever making multiple comparisons like this, it is necessary to control for either family-wise error rate (more stringent) or false discovery rate (less stringent).

```{r}
#| label: fig-trend-map-hatched
#| fig-cap: "Map of slopes with regions of non-significant (alpha = 0.05) slopes marked by hatching.  The p-values have *not* been false-discovery-rate corrected—with FDR correction virtually all slopes are non-significant, especially if using the full non-aggregated dataset."

pval_rast <- rast(lm_df |> select(x, y, p_val)) < 0.05

non_sig <- 
  as.polygons(pval_rast) |>
  filter(p_val == 0)  |>
  mutate(p_val = as.factor(p_val)) 
crs(non_sig) <- crs(gdd_doy_stack_50)

ggplot(lm_df) +
  geom_raster(aes(x = x, y = y, fill = slope)) +
  geom_sf_pattern(
    data = st_as_sf(non_sig),
    aes(pattern_fill = ""), #TODO trick to get legend to show up, but there's a new way to do this in ggplot2 I think
    pattern = "crosshatch",
    fill = NA,
    colour = NA,
    pattern_alpha = 0.5, #maybe not necessary
    pattern_size = 0.05, #make lines smaller
    pattern_spacing = 0.01, #make lines closer together
    pattern_res = 200, #make lines less pixelated
  ) +
  scale_pattern_fill_manual(values = c("grey30")) +
  scale_fill_continuous_diverging(na.value = "transparent", rev = TRUE) +
  labs(fill = "∆DOY/yr",
       pattern_fill = "p > 0.05",
       title = "Trend in DOY to reach 50 GDD",
       x = "",
       y = "") +
  coord_sf(crs = crs(gdd_doy_stack_50)) 
```

### Assumptions

#### ✅ Data are i.i.d. (no temporal autocorrelation)

There doesn't appear to be any temporal autocorrelation in the residuals (@fig-acf), so the data can be treated as i.i.d.

```{r}
#| label: fig-acf
#| fig-cap: "ACF plots for 6 randomly chosen pixels. If there was autocorrelation, the black vertical line segments would reach above or below the dashed blue lines."
#| fig-subcap: ""
#| layout-ncol: 3

set.seed(123)
lm_df |> 
  slice_sample(n = 6) |>
  pull(resid) |> 
  purrr::walk(\(.x) acf(.x))
```

#### ✅ Normality

Residuals are fairly well behaved despite the data being bounded at 0 and 365.
Non-normality is **not** an issue (@fig-resid-samp, @fig-resids).

```{r}
#| label: fig-resid-samp
#| fig-cap: "Q-Q plots for residuals from 6 randomly chosen pixel-wise linear regressions"
set.seed(132)
resid_df <- lm_df |> 
  slice_sample(n = 6) |>
  select(x,y,resid) |> 
  unnest(cols = c(resid)) |> 
  mutate(pixel = paste(x,y, sep = ", ")) 


qqs <- resid_df |> 
  ggplot(aes(sample = resid)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  facet_wrap(vars(pixel)) +
  labs(x = "Theoretical Quantiles", y = "Sample Quantiles")

qqs
```

```{r}
#| label: fig-resids
#| fig-cap: Histogram (A) and Q-Q plot (B) of residuals pooled from all pixel-wise regressions.

resids <- lm_df |> select(resid) |> unnest() 

ggplot(resids) + geom_histogram(aes(resid)) +
ggplot(resids, aes(sample = resid)) + 
  stat_qq(alpha = 0.2) +
  stat_qq_line(color = "red") + 
  labs(x = "Theoretical Quantlie", y = "Sample Quantile")
```

#### ❌ Linearity

These data do *not* meet the assumption that there is a linear relationship between the response (DOY) and the predictor (time), in my opinion.
@fig-linearity shows a few examples of what the time series look like.
One could treat the non-linear fluctuation in these points as error around a trend, but given that we know there are decadal-scale climate fluctuations in addition to a warming trend, I'm not sure that is an appropriate assumption.

```{r}
#| label: fig-linearity
#| fig-cap: Timeseries plots of 5 randomly sampled pixels for 50 GDD threshold
set.seed(34567)
plot_df <- gdd_df |> mutate(pixel = paste0(x,y))
plot_df <- plot_df |> filter(pixel %in% sample(pixel, 5))
plot_df |> 
  ggplot(aes(x = year, y = DOY, group = pixel, color = pixel)) +
  geom_line() +
  theme(legend.position = "none")
  
```

#### ❌ Independence of multiple comparisons (spatial autocorrelation)

While not correcting p-values for multiple comparisons leads to high rates of false positives.
On the other hand, treating multiple tests as fully independent over-corrects and leads to overly conservative p-values [@derringer].
I think it is safe to assume that pixels near each other are more likely to have similar DOY values than pixels far apart, and therefore the hypothesis tests for each pixel are not independent.

False discovery rate corrections typically assume multiple tests are independent (although the `"BY"` method described in `?p.adjust.methods` does not) and base their corrections on the number of p-values which is a function of the somewhat arbitrary resolution of the data.
Down-scaling (as I've done for this example) or up-scaling therefore changes the statistical significance of trends which doesn't make sense since only the representation of the trends is changing.

## Alternative: Spatial GAMs

Generalized Additive Models (GAMs) are an alternative method that involves fitting penalized splines to data.
GAMs might solve our two problems above by allowing for non-linear relationships (both through time and across space) and by accounting for un-modeled short range spatial autocorrelation with a method called neighborhood cross-validation [@wood].
Importantly, this approach uses all of the data in a single model rather than being applied pixel-wise.

GAMs fit penalized smooths to data capturing non-linear relationships with a statistically optimized amount of wiggliness.
Two-dimensional smoothers can be used to capture spatial variation in data (with dimensions of lat/lon).
Furthermore, we can model complex interactions including three-way interactions between latitude, longitude, and time.

```{r}
#| label: tbl-gam-summary
#| tbl-cap: "Output of `summary()` for a spatial GAM"
#| echo: true
#| cache: true
m_reml <- mgcv::bam(
    DOY ~ 
      ti(x, y, bs = "cr", d = 2, k = 25) +                                # <1>
      ti(year_scaled, bs = "cr", k = 10) +                                # <2>
      ti(x, y, year_scaled, d = c(2,1), bs = "cr", k = c(25, 10)),        # <3>
    data = gdd_df,
    method = "REML"
  )
as_flextable(m_reml)
```

1.  Two-dimensional tensor-product smoother for space where both dimensions (lat and lon) are fit using a cubic regression spline (`"cr"`) with 25 knots.
2.  One-dimensional cubic regression spline with 10 knots for change over time
3.  The interaction between the 2D tensor product for space and the smooth for time

From @tbl-gam-summary, we can conclude that there is significant spatial variation in DOY, a significant (non-linear) relationship with time, and a significant interaction between space and time (e.g. not all locations show the same (non-linear) trend).
One can visualize the partial effects of each of these terms in @fig-gam.

```{r}
#| label: fig-gam
#| message: false
#| fig-cap: "Partial effects of space (A), time (B) and their interaction (C) from the best-fit GAM.  In C, the facets are just 6 evenly-spaced samples showing how the spatial effects vary over time from year 0 to 42."
#| fig-height: 8
plots_reml <- draw(m_reml, rug = FALSE, dist = 0.05, n_3d = 6)

((plots_reml[[1]] + coord_sf(crs = crs(gdd_doy_stack_50))) | plots_reml[[2]] ) / (plots_reml[[3]]) + plot_annotation(tag_levels = "A")
```

As you can see in @fig-gam B, the trend through time is not linear.
If we want to extract information from this model to create a map similar to @fig-trend-map-hatched, we can get fitted values for each year of each pixel and calculate *average* slopes and associated p-values testing if those average slopes are different from zero.

::: callout-note
This is the most computationally intensive step.
:::

```{r}
#| label: avg-slopes
#| cache: true
m_reml_slopes <-
  avg_slopes(
    m_reml,
    # newdata = plot_df, #for debug
    variables = "year_scaled",
    type = "response",
    by = c("y", "x"),
    p_adjust = "BY",
    df = insight::get_df(m_reml, type = "model") #TODO: not 100% sure if this is appropriate
  )
```

```{r}
#| label: fig-gam-slopes-map
#| fig-cap: "Map of average slopes of DOY over time from a spatial GAM fit to downsampled data. The p-values in this map have been FDR adjusted."

reml_slopes_df <- m_reml_slopes |> as_tibble()

gam_pval_rast <- 
  rast(m_reml_slopes |> as_tibble() |> select(x, y, p.value)) < 0.05

non_sig <- 
  as.polygons(gam_pval_rast) |>
  filter(p.value == 0)  |>
  mutate(p.value = as.factor(p.value)) 
crs(non_sig) <- crs(gdd_doy_stack_50)

ggplot(reml_slopes_df) +
  geom_raster(aes(x = x, y = y, fill = estimate)) +
  geom_sf_pattern(
    data = st_as_sf(non_sig),
    aes(pattern_fill = ""), #TODO trick to get legend to show up, but there's a new way to do this in ggplot2 I think
    pattern = "crosshatch",
    fill = NA,
    colour = NA,
    pattern_alpha = 0.5, #maybe not necessary
    pattern_size = 0.05, #make lines smaller
    pattern_spacing = 0.01, #make lines closer together
    pattern_res = 200, #make lines less pixelated
  ) +
  scale_pattern_fill_manual(values = c("grey30")) +
  scale_fill_continuous_diverging(na.value = "transparent", rev = TRUE) +
  labs(fill = "∆DOY/yr",
       pattern_fill = "p > 0.05",
       title = "Trend in DOY to reach 50 GDD",
       x = "",
       y = "") +
  coord_sf(crs = crs(gdd_doy_stack_50)) 
```

With the approach in @fig-gam-slopes-map, we have the power to detect significant changes in the DOY to reach 50 GDD over time.
The caveat here is that the relationship fit to time is not linear (see @fig-linearity for an example), and @fig-gam-slopes-map just shows the *average* slopes over time.

### Accounting for spatial autocorrelation

The model described in @tbl-gam-summary is fit with restricted maximum likelihood (REML).
An alternative method, neighborhood cross validation (NCV) could be used to account for un-modeled short-range spatial autocorrelation [@wood].
A requirement for NCV is constructing a list object in R that defines "neighborhoods" around each point.
NCV is a relatively new method (@wood is a preprint) and methods for automated creation of this list object are still in development.

Ideally, when fitting a GAM you should increase the number of knots for a smooth until the estimated degrees of freedom (edf) (the estimated wiggliness) is much less than the number of knots (k).
Basically you want to give the spline enough flexibility that the penalized version can represent the "true" relationship.
With REML and data with spatial autocorrelation there is a risk of overfitting—edf continues to grow as more and more knots are added and the smooth gets far too wiggly.

```{r}
k.check(m_reml)
```

As you can see above, k (24) is very close to edf (23.6) indicating that I haven't given the GAM enough knots in the spatial dimension in this example.
I can tell you that from some haphazard experimenting, overfitting *does* appear to be an issue that could potentially be solved by using NCV instead of REML.

As I get time to play with NCV more, I may add an example of it here for comparison, but I'm still figuring it out at this point.

::: refs
:::
